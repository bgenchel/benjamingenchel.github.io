<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="description" content="Personal Site/Music">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Benjamin Genchel</title>

    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/my-carousel.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/baguettebox.js/1.8.1/baguetteBox.min.css">
    <link rel="stylesheet" href="css/gallery-grid.css">
    <link rel="stylesheet" href="css/sidenav.css">
    <link rel="stylesheet" href="css/style.css">
    <link href="https://fonts.googleapis.com/css?family=Questrial" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
    <link rel='shortcut icon' type='image/x-icon' href='favicon.ico'/>
</head>
<body>
    <div id="mynavbar">
        <div id="mynavbar-branding">
            <img src="res/8bit-self.png" id="mynavbar-pic">
            <span id="mynavbar-name">Benjamin Genchel</span>
        </div>
        <div id="menubtn-bg-holder">
            <span id="menubtn-bg">&#9776;   Menu</span>
        </div>
        <div id="mynavbar-links">
            <span id="menubtn" class="nl-menubtn" onClick="openNav()">&#9776;   Menu</span>
        </div>
    </div>
    <div id="mySidenav" class="sidenav">
        <a href="javascript:void(0)" class="closebtn" onClick="closeNav()">&times</a>
        <a class="mynavbutton" href="index.html">Home</a>
        <a class="mynavbutton" href="#">Projects</a>
        <a class="mynavbutton" href="sounds.html">Sounds</a>
        <a class="mynavbutton" href="sheets.html">Sheets</a>
        <a class="mynavbutton" href="res/Benjamin_Genchel_Resume.pdf" download>Resume</a>
        <a class="mynavbutton" href="contact.html">Contact</a>
        <div class="sn-sml-holder">
            <a class="sn-sml" href="http://soundcloud.com/bgenchel">
                <img id="smb-soundcloud" class="sn-smb" src="css/res/icons/soundcloud.svg">
            </a>
            <a class="sn-sml"href="http://linkedin.com/in/benjamingenchel">
                <img id="smb-linkedin" class="sn-smb" src="css/res/icons/linkedin.svg">
            </a>
            <a class="sn-sml" href="http://github.com/bgenchel">
                <img id="smb-github" class="sn-smb" src="css/res/icons/github.svg"></a>
            </a>
        </div>
    </div>
    <div style="height: 4em"></div>
    <div id="page-title">
        <h1 id="title">Projects</h1>
        <!-- <div id="title-frame"></div> -->
        <div id="projects" class="parallax"></div>
    </div>
    <div class="container-fluid subsection-container">
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="tz-gallery">
                    <div class="gallery-img">
                        <a class="lightbox" href="res/DRL_4MG_MidtermReport.pdf" download> 
                            <img class="projects-report sheet_music" src="res/DRL_4MG.jpg" alt="DRL_4MG">
                        </a>
                        <p class="lightbox-caption">Click to Download (7 Pages)</p>
                    </div>
                </div>
            </div>
            <div class="col-6 item-col">
                <div class="item-title">
                    <h3>Exploring Deep Reinforcement Learning<br>
                        for Symbolic Music Generation<br>(2018 - )</h3>
                </div>
                <p>
                    Deep Learning approaches to music generation have become increasingly popular, but modern systems
                    lack the ability to learn and produce coherent long-term structure in music &mdash; these systems do
                    not understand what they are learning, and as such are not able to use their knowledge to
                    communicate musically, or act musically creative in the way a human would. One of 
                    the most fascinating things to me about conducting machine learning and generative systems research, 
                    and specifically conducting this research with respect to music, is that I get to grapple with these types
                    of problems which bridge engineering with an understanding of our own learning processes. What does 
                    it mean to communicate musically? What does it mean to be creative in a musical fashion? How does 
                    musical learning happen? What types of processing and memory are involved in music learning, and how can we model it?
                    <br><br>
                    Learning heirarchical long term structure appears key to moving these systems, and thus our
                    understanding of these questions, forward. For my Masters Project at GTCMT, I chose to apply 
                    Deep Reinforcement Learning (DRL) to music generation due to its recent success in learning long term
                    structure for complex strategy games such as Go, Shogi, League of Legends and StarCraft. Like these games, music is a
                    task which provides sparse rewards (you don't know whether a musical phrase is good just from
                    hearing a single note &mdash; you have to hear the whole thing), and a task which contains multiple
                    heirarchical levels of structure. 
                    <br><br>
                    Alpha Zero, Google DeepMind's extraordinary DRL system that mastered multiple games, has also
                    been described as playing in a creative fashion.
                </p>
                <blockquote><p>
                    “Impressively, it manages to impose its style of play across a very wide range of positions and 
                    openings,” says Matthew [Sadler], who also observes that it plays in a very deliberate style from its first move with a “very human sense of consistent purpose”.

                    “Traditional engines are exceptionally strong and make few obvious mistakes, but can drift when faced with positions
                    with no concrete and calculable solution,” he says. “It's precisely in such positions where ‘feeling’, ‘insight’ or
                    ‘intuition’ is required that AlphaZero comes into its own."<br><br>
                    &mdash; 
                    <a href="https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/">
                        DeepMind, AlphaZero: Shedding New Light ...</a>
                </p></blockquote>

                DRL has been scarcely applied to music due to its abstract and subjective nature; how can you provide
                reward a system for making good music when what makes music good is so unclear? However, I still believe
                there is a lot of promise in this approach, especially with the development of techniques such as adversarial
                learning, curiosity and inverse reinforcement learning, each of which allows models to learn reward
                functions in different ways. 

                This project does not cover all I am interested in exploring with respect to DRL, but represents a first step into
                the field for me. It aims to combines two previous sequence generation approaches which use DRL, 
                <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14344">SeqGAN</a> and <a
                   href="https://openreview.net/forum?id=Syyv2e-Kx">TunerRL</a>. My current progress is detailed in the
               document posted here.
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="item-title">
                    <h3>Explicitly Conditioniong Melody Generation:<br>
                        A Case Study with Interdependent RNNs<br>(2018 - 2019)</h3>
                </div>
                <p>
                    This project examines the effects of explicitly conditioning a deep learning music generation system 
                    with various musical factors. Explicit conditioning here refers to deriving specific,
                    human-parsable information from the data (e.g. chords, note durations, note pitches, note positions
                    relative to a bar) from the data, processing them separately, and then feeding them to the model
                    along side the primary information to be modelled. The model in question consists of two parallel
                    LSTM-RNNs (Long Short Term Memory - Recurrent Neural Network), one that models and generates note
                    pitch sequences, and the other that models and generates note duration sequences. The outputs of
                    each are combined 1 to 1 to form a monophonic melody. 
                    <br><br>
                    This project initially began with the model architecture, which I conceived while trying to think
                    of an appropriate baseline for another idea for my graduate research project for the 2018. As a jazz
                    musician, I tend to think of music as a set of separate but co-dependent parts, such as harmony,
                    melody, rhythm, that are embodied with finer detail in an arrangement or performance. Lead sheets
                    provide these core components, with section markers, overall feels (e.g. swing, bop), chord labels, 
                    key changes, etc. and musicians and bands interpret them in their own way. 
                    As such, I wanted to pursue a project that would reflect and explore that line of thinking. I continued
                    development on this project while working as a visiting researcher in the 
                    <a href="https://musicai.citi.sinica.edu.tw/">Music and A.I. Lab</a> in Taipei, Taiwan over the
                    following summer, and began collaborating with my colleague 
                    <a href="https://ashispati.github.io/">Ashis Pati</a> to evaluate it and publish the
                    results.
                    <br><br>
                    I published the model architecture of this work as a late breaking abstract at the 
                    <a href="https://csmc2018.wordpress.com/programme/">Computer Simulation of Musical Creativity
                    Conference in 2018</a>, and plan to submit a paper on the results of using the model to compare
                     the effects of explicit conditioning factors to the 2019 
                    <a href="http://musicalmetacreation.org/">Musical Metacreation Workshop</a>.
            </div>
            <div class="col-6 item-col">
                <div class="tz-gallery">
                    <div class="gallery-img" style="padding: 5%;">
                        <a class="lightbox"> 
                            <img src="res/min_model.jpg" alt="min_model" style="width: 100%;">
                        </a>
                        <p class="lightbox-caption">Model Architecture</p>
                    </div>
                    <p style="margin-top: 1em;"> 
                        Model architecture overview — This figure displays the all different musical data series used in the
                        model, and how they are processed and used for conditioning. In particular, this figure shows
                        all factors being applied simulataneously; one configuration of many that we tested. Other 
                        configurations can be derived from this one by selectively removing a subset of connections 
                        prior to the embedding concatenation operation.
                    </p>
                </div>
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="tz-gallery">
                    <div class="gallery-img">
                        <a class="lightbox" href="res/mpc_report.pdf" download> 
                            <img class="projects-report sheet_music" src="res/mpc_report.jpg" alt="mpc_report">
                        </a>
                        <p class="lightbox-caption">Click to Download (11 Pages)</p>
                    </div>
                </div>
            </div>
            <div class="col-6 item-col">
                <div class="item-title"><h3>What Makes Machine Music Machine Like? (2018)</h3></div>
                <blockquote><p>
                    &nbsp;&nbsp;&nbsp;&nbsp;Can humans truly tell the difference between music composed by humans and music generated by a 
                    modern deep learning based statistical model? Without the bias of knowing whether the music they 
                    hear is composed by a machine or a human, what characteristics would people identify as human like,
                    or machine like?<br><br>
                    Generally speaking, it appears intuitive that it would be easy for most people to differentiate a 
                    piece of music produced by a human from one produced by a modern deep learning model. One need only listen 
                    to a substantially long generation from any of these systems to intuit that the layman who inevitably 
                    asks if researchers are attempting to replace human musicians has nothing to fear at present,
                    [...]. In the event that making such a classification is in fact as easy as it appears, 
                    we would not know for certain, as the majority of evaluations presented in this area compare their 
                    model’s output to a set of baseline models along a set of objective (though contestable) metrics or 
                    via a listening test in which each contestant is rated along preset dimensions. 
                    Thus, there has been little work in the way of comparing machine generated music to actual music; 
                    characterizing the perceptual differences between the two and discovering how people listen and 
                    what they listen for in the context of these studies.
                </p></blockquote>
                <p>
                    In this pilot study, my final project for the Music Perception and Cognition class at Georgia Tech,
                    my teammates and I surveyed a small population of students in the course (~14) to gain qualitative
                    insight into the perceived differences between real melodies and machine generated melodies. We
                    presented each subject with pairs of melodies, one real and one generated, played over the same set
                    of chords and asked them to provide both quantitative ratings and qualitative responses. 

                    The results of this study are presented in the paper provided here. I hope to pursue a full, formal version 
                    of this study in the near future.
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="item-title"><h3>PyPadberg (2018)</h3></div>
                <p>
                    In 1964, Harriet Padberg completed the first dissertation on algorithmic composition using a
                    computer, <i>Computer Composed Canon and Free Fugue</i>, which converted text inputs into melodies using
                    a custom algorithm. This work stood out at the time for using a text-to-music mapping instead
                    of the random number generation based systems used in her contemporaries’
                    pieces, e.g. Illiac Suite by Lejaren Hiller. It received moderate attention in the years
                    following its creation, including a 1968 sonic realization by Max Matthews and mention by Hiller in a 1970
                    survey of computer-generated music. In the present, her work has largely been forgotten; it is not 
                    commonly known, nor spoken about outside of select papers, presenting not only a lack of attention 
                    towards a significant historical work, but also the loss of opportunity to learn from and adapt it 
                    for new musical creations.
                    <br><br>
                    In this project, my teammates <a href="http://richardsavery.com/">Richard Savery</a>, Jason Smith, and I 
                    dug up all the information we could on Padberg to develop our own 
                    <a href="https://github.com/bgenchel/pypadberg">recreation in python</a>.
                    We then reached out to three composers to create compositions using the system in whatever way they saw fit.
                    The results were three widely varied and unique statements on feminism, alternative tuning systems
                    and the use of text in music.  
                    <br><br>
                    This work has been accepted to the "Women, Music, Technologies" session by the Committee on the
                    Status of Woman of the Society for Music Theory at the 2019 Society for Music Theory meeting.
                    
            </div>
            <div class="col-6 item-col">
                <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
                    <ol class="carousel-indicators">
                        <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
                        <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
                        <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
                        <li data-target="#carouselExampleIndicators" data-slide-to="3"></li>
                    </ol>
                    <div class="carousel-inner">
                        <div class="carousel-item active" style="height: fit-content;">
                            <img class="d-block w-100" src="res/pypadberg/splash.png" alt="First slide">
                        </div>
                        <div class="carousel-item" style="height: fit-content;">
                            <img class="d-block w-100" src="res/pypadberg/text_entry.png" alt="Second slide">
                        </div>
                        <div class="carousel-item" style="height: fit-content;">
                            <img class="d-block w-100" src="res/pypadberg/processing.png" alt="Third slide">
                        </div>
                        <div class="carousel-item" style="height: fit-content;">
                            <img class="d-block w-100" src="res/pypadberg/rendering.png" alt="Fourth Slide">
                        </div>
                    </div>
                    <a class="carousel-control-prev" href="#carouselExampleIndicators" role="button" data-slide="prev">
                        <span class="carousel-control-prev-icon" aria-hidden="true"></span>
                        <span class="sr-only">Previous</span>
                    </a>
                    <a class="carousel-control-next" href="#carouselExampleIndicators" role="button" data-slide="next">
                        <span class="carousel-control-next-icon" aria-hidden="true"></span>
                        <span class="sr-only">Next</span>
                    </a>
                </div>
                <p style="margin-top: 1em;"> 
                    PyPadberg Interface &mdash; this slideshow displays the interface of the PyPadberg program in order of
                    appearance: Title Screen, Text Entry, Text Processing, Rendering. The title screen shows a woman working
                    on the IBM 1620 computer, the same computer Harriet Padberg used for her dissertation.
                </p>
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
            </div>
            <div class="col-6 item-col">
                <div class="item-title"><h3>Musical Painting Arm (2018 - )</h3></div>
                <p>
                    Description coming soon!
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="item-title"><h3>Sound Happening (2018)</h3></div>
                <p> Sound Happening is an ongoing interactive musical installation project housed by the 
                <a href="https://expressivemachinery.gatech.edu/">Expressive Machinery Lab</a> at Georgia Tech. Its
                purpose is to allow participants to engage in an active, collaborative, and most importantly playful
                musical experience. Designed to be easy to take and install anywhere, it consists of only a webcam, a
                laptop to run color tracking software and send messages to Ableton Live, and colorful bouncy balls.  
                <br><br>
                During the Spring of 2018, I worked on Sound Happening alongside my colleague 
                <a href="http://richardsavery.com/">Richard Savery</a>. We implemented new functionality, better,
                cleaned the code base, and developed new sounds and loops. For my own sound design, I focused on using
                randomness such that the sounds produced by the motion of each ball would be different each time. 
                <br><br> 
                A video explaining the concept of Sound Happening (made prior to our work), as well as a video showing 
                our progress are presented here.
            </div>
            <div class="col-6 item-col" style="text-align: center;">
                <iframe class="youtube-vid" width="560" height="315" src="https://www.youtube-nocookie.com/embed/8sUmnQvu7bQ"
                    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
                <iframe class="youtube-vid" width="560" height="315" src="https://www.youtube-nocookie.com/embed/VqLmMAHYCBc"
                    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col" style="text-align: center;">
                <iframe src="https://open.spotify.com/embed/track/4Y8r1jX0Ei9G3Ky7cYt7ep" width="300" height="380" 
                    frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>           
            </div>
            <div class="col-6 item-col">
                <div class="item-title"><h3>Evolving Towards Green Sleeves (2018)</h3></div>
                <p>
                    Evolving Towards Green Sleeves is a computer music composition which sonifies the process of a
                    machine learning system learning how to play a song. Specifically, I implemented an evolutionary
                    algorithm from scratch in python, and had it overfit on the melody to green sleeves, represented as 
                    parallel midi note number and duration token sequences. I trained the system for 700 iterations,
                    with 350 candidates generated and 40 candidates selected per generation. Starting with a totally
                    random set of candidates, each iteration the system computes the mean squared error between each
                    candidate and the ground truth, keeps the top 40 candidates with the lowest losses and throws away the
                    rest, generating a new set of 350 candidates via random sampling from the selected set. A random
                    gaussian mutation is added to the new candidates.
                    <br><br>
                    I tracked the average loss during training and selected random candidates from the pool every time
                    the loss would decrease by a relatively significant amount. The core of the final piece is the result of
                    concatenating these in order. I wanted to emphasize a notion of moving from chaos to order, and so
                    near the beginning, had several samples playing over each other with heavy reverb / distortion while near the end,
                    I selected only one or two melodies to play with clarity. 
                    <br><br>
                    Code for the project can be found 
                    <a href="https://github.com/bgenchel/Evolving-Towards-Green-Sleeves">here</a>
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="item-title"><h3>Breadboard MIDI Controller (2018)</h3></div>
                <p>
                    This device is a custom built MIDI controller using a wide range of sensors and inputs over four
                    breadboards, motivated by an assignment for a class at Georgia Tech which asked students to create a
                    custom MIDI inputs to MaxMSP. While the assignment asked for much less, I was really motivated
                    to build a full controller, inspired by all the different varieties of MIDI controllers I had been
                    seeing both inside and outside the Music Technology program. 
                    <br><br>
                    The controller, built on an Arduino Uno, sends messages over serial which are read by Max/MSP, then
                    forwarded to Ableton Live over OSC. The class was so impressed with my design that I was invited to perform with it at the 2018 Guthman
                    Musical Instrument Competition pre-show. While I myself am not experienced performing with MIDI
                    controllers, the breadboard controller served as a point of interest for attendees, who frequently
                    visited my booth to play with it and ask questions. 
                    <br><br>
                    The first video shown here is a walk through of
                    the controller. The second, a short clip from the Guthman pre-show (I used a launchpad to record,
                    store and launch loops created with the controller).
                </p>
            </div>
            <div class="col-6 item-col" style="text-align: center;">
                <iframe class="youtube-vid" width="560" height="315" src="https://www.youtube-nocookie.com/embed/TAraRy9DHk4"
                    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                                    allowfullscreen></iframe>
                <iframe class="youtube-vid" width="560" height="315" src="https://www.youtube-nocookie.com/embed/qJmrfS5dx8o"
                    frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="tz-gallery">
                    <div class="gallery-img">
                        <a class="lightbox" href="res/2017proposal.pdf" download> 
                            <img class="projects-report sheet_music" src="res/2017proposal.jpg" alt="proposal">
                        </a>
                        <p class="lightbox-caption">Click to Download (23 Pages)</p>
                    </div>
                </div>
            </div>
            <div class="col-6 item-col">
                <div class="item-title"><h3>NSF Grant Proposal:<br>A Novel Deep Learning Model for Musical Creativity (2017)</h3></div>
                <p>
                This grant proposal was written by my colleagues <a href="https://richardyang40148.github.io/">Li-Chia
                    "Richard" Yang</a>, <a href="https://ashispati.github.io//contact/">Ashis Pati</a>, and me during my first
                semester at Georgia Tech on behalf of professors <a href="https://www.linkedin.com/in/weinberggil/">Gil
                    Weinberg</a> and <a href="https://www.alexanderlerch.com/">Alexander Lerch</a>. It describes a novel
                deep learning system which would be trained using information from both scores and semantically
                meaningful perceptual and performance features extrated from audio recordings of performances of said scores. 
                It would also use detailed, semantically meaningful score annotations provided by experts. Positioned squarly 
                as research towards computational creativity, rather than higher quality generation for other reasons, 
                this proposal targeted Jazz improvisation generation as a genre of music for which creativity was more
                necessary and from which creativity could be more easily detected and evaluated. 
                <br><br>
                Though this proposal was rejected, it was a valuable experience for me to have especially at the
                beginning of my Master's program. I learned a lot about proposal writing, and deep learning for
                creativity and music generation through the process.
            </div>
        </div>
        <hr>
        <div class="row item-row">
            <div class="col-6 item-col">
                <div class="item-title"><h3>.dotfiles (2016 - )</h3></div>
                I am a huge fan of the terminal. My workflow for any software project, and even some writing
                assignments, uses TMUX, Vim, and a ton of plugins. I also use zsh as my shell, with Oh-My-Zsh as my
                plugin manager. 
                <br><br>
                I've collected all my dotfiles in a repository, and provided some installation scripts to help both me
                (when I gain access to a new server, or refresh my computer) and friends interested in adopting the
                terminal workflow get everything needed in place quickly.
                <br><br>
                Special thanks to <a href="https://www.linkedin.com/in/alexyang2818/">Alex Yang</a>, who helped me get
                started with all this stuff.
                <br><br>
                <a href="https://github.com/bgenchel/.dotfiles.git">Check out my .dotfiles!</a>
            </div>
            <div class="col-6 item-col">
                <div class="tz-gallery">
                    <div class="gallery-img" style="padding: 5%;">
                        <a class="lightbox"> 
                            <img src="res/dotfiles.png" alt="terminal">
                        </a>
                        <p class="lightbox-caption">A Glimpse of My Workflow</p>
                    </div>
                </div>
                <p>
                    Screenshot of my terminal - I use iterm2, tmux, vim, and zsh with the myagnoster theme. You
                    can see here that I use command highlighting and auto complete plugins for zsh, and the
                    YouCompleteMe plugin for vim. I'm also a permanent user of the monokai colorscheme. 
                </p>
            </div>
        </div>
        <hr>
        <div class="row item-row no-viz-col">
            <div class="item-title" style="width: 100%"><h3>Web Development<br class="mobile-br">(2014 - )</h3></div>
            <p>
                In 2013, I volunteered to take on the position of Webmaster for my professional engineering
                fraternity, Theta Tau. Though an Electrical Engineering major, I knew I had a knack for coding from
                my introductory C class and MatLab, and wanted a reason to learn more. I got the job (no one else
                wanted it), and remained in the position for two years, struggling tooth and nail against an
                avalanche of my own ignorance and no one to guide me. Several years later, with many attempts of
                varying degrees of success later, I am somewhat competent. 
                <br><br> 
                I've built many pages, both from scratch and from templates, using jquery, bootstrap, vanilla 
                javascript, and a variety of plugins. I've also worked on backend development, with Google App Engine, 
                AWS, PostgreSQL, MySQL, NodeJS and Express.
                <br><br>
                Though Wix and SquareSpace make great looking sites in a short amount of time, I always challenge
                myself to make my own. This page was built from scratch using JQuery and Bootstrap, and I have 
                plans to make it a kind of instrument using Tone.JS in the near future.
            </p>
        </div>
    </div>
    <div id="footer">
        <p>Created with Bootstrap / JQuery | © Benjamin Genchel</p>
    </div>
    <script src="js/sidenav.js"></script>
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
</body>
</html>
